import discord
from discord.ext import commands
import asyncio
import concurrent.futures
from diffusers import StableDiffusionPipeline
import os
import torch
from torch.amp import autocast
from transformers import AutoModelForCausalLM, AutoTokenizer
from Backend.send import send

torch.backends.cudnn.benchmark = True
torch.backends.cudnn.enabled = True

class AI(commands.Cog):
    def __init__(self, bot):
        self.bot = bot
        
        self.pipeline = StableDiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-2-1",
            torch_dtype=torch.float16
        )
        
        self.pipeline.enable_model_cpu_offload()
        self.pipeline.enable_vae_slicing()
        model_name = "nvidia/Llama-3.1-Nemotron-70B-Instruct-HF"
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16,
            device_map="auto"
        )
        self.message_history = {}
        torch.cuda.empty_cache()

    def generate_response(self, user_id, question):
        if user_id not in self.message_history:
            self.message_history[user_id] = []
        self.message_history[user_id].append({"role": "user", "content": question})
        if len(self.message_history[user_id]) > 10:
            self.message_history[user_id] = self.message_history[user_id][-10:]
        messages = "\n".join(
            f"{msg['role']}: {msg['content']}" for msg in self.message_history[user_id]
        )
        inputs = self.tokenizer(messages, return_tensors="pt", padding=True, truncation=True).to(self.model.device)
        outputs = self.model.generate(**inputs, max_length=2048, do_sample=True, top_p=0.95, temperature=0.7)
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        self.message_history[user_id].append({"role": "assistant", "content": response})
        return response

    @commands.command(description="Ask an AI a question")
    async def ask(self, ctx, question):
        user_id = ctx.author.id
        loading_message = await send(self.bot, ctx, title='Generating response', content="Please wait while the bot generates a response", color=0x2ECC71)            
        try:
            with concurrent.futures.ThreadPoolExecutor() as pool:
                async with ctx.typing():
                    response = await asyncio.get_event_loop().run_in_executor(
                        pool,
                        lambda: self.generate_response(user_id, question)
                    )
            if len(response) > 4096:
                chunks = [response[i:i+4096] for i in range(0, len(response), 4096)]
                for chunk in chunks:
                    await send(self.bot, ctx, title=f'{question}', content=chunk, color=0x2ECC71)
            else:
                await send(self.bot, ctx, title=f'{question}', content=response, color=0x2ECC71)
        except Exception as e:
            await send(self.bot, ctx, title='Error', content=f"An error occurred generating the response: {str(e)}", color=0xFF0000)
        finally:
            await loading_message.delete()

    @commands.command(description="Generate an image", aliases=["img", "imagine"])
    async def generate(self, ctx, prompt):
        loading_message = await send(self.bot, ctx, title='Generating image', content="Generating image... This may take a while. You'll be notified when it's done.")
        try:
            with concurrent.futures.ThreadPoolExecutor() as pool:
                try:
                    with autocast('cuda'):
                        image = await asyncio.get_event_loop().run_in_executor(
                            pool,
                            lambda: self.pipeline(
                                prompt,
                                num_inference_steps=15
                            ).images[0]
                        )
                except torch.cuda.OutOfMemoryError:
                    print("Not enough GPU memory. Trying CPU fallback.")
                    self.pipeline.to('cpu')
                await loading_message.delete()
                
                image.save(f'{ctx.author.id}.png')
                message = await send(self.bot, ctx, title=f'Image generated!', content=f'{prompt} | Generated by {ctx.author.mention}', image=discord.File(f'{ctx.author.id}.png'))
                os.remove(f'{ctx.author.id}.png')
                torch.cuda.empty_cache()
        except Exception as e:
            await ctx.send(f"An error occurred: {str(e)}")
        finally:
            await loading_message.delete()

async def setup(bot):
    await bot.add_cog(AI(bot))