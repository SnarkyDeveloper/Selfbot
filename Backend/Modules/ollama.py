import discord
from discord.ext import commands
from ollama import chat
from ollama import ChatResponse
import asyncio
import concurrent.futures
from diffusers import StableDiffusionPipeline
import os
import torch
from torch.amp import autocast
from Backend.send import send
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.enabled = True


class Ollama(commands.Cog):
    def __init__(self, bot):
        self.bot = bot
        self.pipeline = StableDiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-2-1",
            torch_dtype=torch.float16
        )
        
        self.pipeline.enable_model_cpu_offload()
        self.pipeline.enable_vae_slicing()
        torch.cuda.empty_cache()

    @commands.command(description="Ask an AI a question")
    async def ask(self, ctx, question):
        loading_message = await send(self.bot, ctx, title='Generating response', content="Please wait while the bot generates a response", color=0x2ECC71)            
        with concurrent.futures.ThreadPoolExecutor() as pool:
            async with ctx.typing():
                response = await asyncio.get_event_loop().run_in_executor(
                    pool,
                    lambda: chat(
                        model='llama3.2',
                        messages=[{'role': 'user', 'content': question}]
                    )
                )
        message = response.message.content
        try:
            if len(message) > 4096:
                chunks = []
                for i in range(0, len(message), 4096):
                    chunks.append(message[i:i+4096])
                for chunk in chunks:
                    await send(self.bot, ctx, title=f'{question}', content=chunk, color=0x2ECC71)
            else:
                await send(self.bot, ctx, title=f'{question}', content=message, color=0x2ECC71)
        except Exception as e:
            await send(self.bot, ctx, title='Error', content=f"An error occurred generating the response: {str(e)}", color=0xFF0000)
        finally:
            await loading_message.delete()

    @commands.command(description="Generate an image", aliases=["img", "imagine"])
    async def generate(self, ctx, prompt):
        loading_message = await send(self.bot, ctx, title='Generating image', content="Generating image... This may take a while. You'll be notified when it's done.")
        try:
            with concurrent.futures.ThreadPoolExecutor() as pool:
                try:
                    with autocast('cuda'):
                        image = await asyncio.get_event_loop().run_in_executor(
                            pool,
                            lambda: self.pipeline(
                                prompt,
                                num_inference_steps=15
                            ).images[0]
                        )
                except torch.cuda.OutOfMemoryError:
                    print("Not enough GPU memory. Trying CPU fallback.")
                    self.pipeline.to('cpu')
                await loading_message.delete()
                
                image.save(f'{ctx.author.id}.png')
                message = await send(self.bot, ctx, title=f'Image generated!', content=f'{prompt} | Generated by {ctx.author.mention}', image=discord.File(f'{ctx.author.id}.png'))
                os.remove(f'{ctx.author.id}.png')
                torch.cuda.empty_cache()
        except Exception as e:
            await ctx.send(f"An error occurred: {str(e)}")
        finally:
            await loading_message.delete()

async def setup(bot):
    await bot.add_cog(Ollama(bot))
