import discord
from discord.ext import commands
from ollama import chat
from ollama import ChatResponse
import asyncio
import concurrent.futures
from diffusers import StableDiffusionPipeline
import os
import torch
from torch.amp import autocast
from Backend.send import send
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.enabled = True

async def loading(ctx, interval=0.5, type="text"):
    if type == "image":
        x = await ctx.send("Generating image... This may take a while. You'll be notified when it's done.")
    else:
        x = await ctx.send("Generating response...")
    load = await ctx.send("**↺**")
    try:
        while True:
            await load.edit(content="**↺**")
            await asyncio.sleep(interval)
            await load.edit(content="**⟲**") 
            await asyncio.sleep(interval)
    finally:
        await x.delete()
        await load.delete()

class Ollama(commands.Cog):
    def __init__(self, bot):
        self.bot = bot
        self.pipeline = StableDiffusionPipeline.from_pretrained(
            "stabilityai/stable-diffusion-2-1",
            torch_dtype=torch.float16
        )
        
        self.pipeline.enable_model_cpu_offload()
        self.pipeline.enable_vae_slicing()
        torch.cuda.empty_cache()

    @commands.command(description="Ask an AI a question")
    async def ask(self, ctx, question):
        loading_message = await send(self.bot, ctx, title='Generating response', content="Please wait while the bot generates a response", color=0x2ECC71)            
        with concurrent.futures.ThreadPoolExecutor() as pool:
            response = await asyncio.get_event_loop().run_in_executor(
                pool,
                lambda: chat(
                    model='llama3.2',
                    messages=[{'role': 'user', 'content': question}],
                    stream=True
                )
            )
        await loading_message.delete()
        try:
            await send(self.bot, ctx, title=f'{question}', content=response['replies'][0]['content'], color=0x2ECC71)
        except Exception as e:
            await send(self.bot, ctx, title='Error', content=f"An error occurred: {str(e)}", color=0xFF0000)

    @commands.command(description="Generate an image", aliases=["img", "imagine"])
    async def generate(self, ctx, prompt):
        loading_message = await send(self.bot, ctx, title='Generating image', content="Generating image... This may take a while. You'll be notified when it's done.")
        try:
            with concurrent.futures.ThreadPoolExecutor() as pool:
                try:
                    with autocast('cuda'):
                        image = await asyncio.get_event_loop().run_in_executor(
                            pool,
                            lambda: self.pipeline(
                                prompt,
                                num_inference_steps=15
                            ).images[0]
                        )
                except torch.cuda.OutOfMemoryError:
                    print("Not enough GPU memory. Trying CPU fallback.")
                    self.pipeline.to('cpu')
                await loading_message.delete()
                
                image.save(f'{ctx.author.id}.png')
                message = await send(self.bot, ctx, title=f'Image generated!', content=f'{prompt} | Generated by {ctx.author.mention}', image=discord.File(f'{ctx.author.id}.png'))
                os.remove(f'{ctx.author.id}.png')
                torch.cuda.empty_cache()
        except Exception as e:
            while_loading = False
            await loading_message.delete()
            await ctx.send(f"An error occurred: {str(e)}")

async def setup(bot):
    await bot.add_cog(Ollama(bot))
